#!/usr/bin/env python3
from typing import Dict, List
import json
import time
import os
import sys
import signal
import argparse
from datetime import datetime
from collections import Counter, defaultdict
import pandas as pd
import matplotlib.pyplot as plt
from dotenv import load_dotenv
from openai import OpenAI
import queue
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

load_dotenv()


class ReviewAnalyzer:
    def __init__(self, api_key: str = None):
        """
        åˆå§‹åŒ–DeepSeek AIåˆ†ç±»å™¨

        Args:
            api_key: DeepSeek APIå¯†é’¥ï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
        """
        # åªæ”¯æŒDeepSeek
        self.api_key = api_key or os.getenv("DEEPSEEK_API_KEY")
        if not self.api_key:
            raise ValueError(
                "DeepSeek APIå¯†é’¥æœªæ‰¾åˆ°ï¼è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®DEEPSEEK_API_KEY"
            )

        self.client = OpenAI(api_key=self.api_key, base_url="https://api.deepseek.com")
        self.model = "deepseek-chat"

        print(f"ğŸ¤– ä½¿ç”¨DeepSeek API ({self.model})")

        # è¿›åº¦ä¿å­˜ç›¸å…³
        self.checkpoint_file = None
        self.auto_save_interval = int(
            os.getenv("AUTO_SAVE_INTERVAL", "10")
        )  # æ¯10æ¡ä¿å­˜ä¸€æ¬¡

        # APIé”™è¯¯è®¡æ•°
        self.consecutive_failures = 0
        self.max_consecutive_failures = 3

        # å¹¶è¡Œå¤„ç†é…ç½®
        self.parallel_workers = int(os.getenv("PARALLEL_WORKERS", "5"))
        self.request_delay = float(os.getenv("REQUEST_DELAY", "0.1"))

        # çº¿ç¨‹å®‰å…¨çš„é”å’Œåœæ­¢æ ‡å¿—
        self.progress_lock = threading.Lock()
        self.stop_flag = threading.Event()

        # è®¾ç½®ä¿¡å·å¤„ç†å™¨
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)

        # å®šä¹‰åˆ†ç±»ä½“ç³»
        self.categories = {
            "positive": {
                "å‰§æƒ…æ•…äº‹": "æ¸¸æˆçš„å†å²èƒŒæ™¯ã€æ•…äº‹æƒ…èŠ‚ã€æ–‡å­¦æ€§ã€å™äº‹ç­‰æ–¹é¢",
                "ç¾æœ¯éŸ³æ•ˆ": "ç”»é¢ã€éŸ³ä¹ã€è§†è§‰æ•ˆæœã€éŸ³å“æ•ˆæœç­‰è‰ºæœ¯è¡¨ç°",
                "æ¸¸æˆæ€§": "ç©æ³•åˆ›æ–°ã€æ“ä½œä½“éªŒã€æ¸¸æˆè®¾è®¡ç­‰æœºåˆ¶å±‚é¢",
                "æƒ…æ„Ÿå…±é¸£": "æ„ŸåŠ¨ã€æƒ…æ€€ã€å¯¹å›½äº§æ¸¸æˆçš„æ”¯æŒç­‰æƒ…æ„Ÿå› ç´ ",
                "å…¶ä»–": "æ— å…·ä½“ç†ç”±çš„å¥½è¯„",
            },
            "negative": {
                "æ¸¸æˆè´¨é‡": "ä¼˜åŒ–é—®é¢˜ã€bugã€å¡é¡¿ã€é—ªé€€ã€æ€§èƒ½ç­‰æŠ€æœ¯é—®é¢˜",
                "æ¸¸æˆå†…å®¹": "ä¸å¥½ç©ã€å¤ªéš¾ã€åœ°å›¾è®¾è®¡ã€bossè®¾è®¡ã€æ“ä½œæ‰‹æ„Ÿã€UIç­‰ç©æ³•ä½“éªŒé—®é¢˜",
                "å†å²äº‰è®®": "æ»¡æ¸…ç¼ºå¸­ã€å†å²è€ƒæ®ã€æ•æ„Ÿå†…å®¹ã€å²å®å‡†ç¡®æ€§ç­‰å†å²ç›¸å…³äº‰è®®",
                "å®£å‘é—®é¢˜": "è¥é”€ç‚’ä½œã€505å‘è¡Œã€å®šä»·ç­–ç•¥ã€è¯•ç©å·è·‘ã€é¢„è´­ç­‰å®£ä¼ å‘è¡Œé—®é¢˜",
                "åç»­å…¬å…³": "è±ªåç‰ˆè¡¥å¿ã€ä¼˜åŒ–æ•ˆç‡ã€å®˜æ–¹æ€åº¦ç­‰å‘å”®åæœåŠ¡é—®é¢˜",
                "å…¶ä»–": "æ— å…·ä½“ç†ç”±çš„å•çº¯è°©éª‚æˆ–æƒ…ç»ªå‘æ³„",
            },
        }

    def test_api_connection(self) -> bool:
        """æµ‹è¯•DeepSeek APIè¿æ¥"""
        print("ğŸ” æ­£åœ¨æµ‹è¯•DeepSeek APIè¿æ¥...")

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant"},
                    {"role": "user", "content": "è¯·å›å¤'è¿æ¥æˆåŠŸ'"},
                ],
                max_tokens=10,
                temperature=0.1,
            )

            content = response.choices[0].message.content.strip()
            print("âœ… DeepSeek APIè¿æ¥æµ‹è¯•æˆåŠŸ")
            print(f"å“åº”å†…å®¹: {content}")
            return True

        except Exception as e:
            print(f"âŒ DeepSeek APIè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False

    def _signal_handler(self, signum, frame):
        """å¤„ç†ä¸­æ–­ä¿¡å·ï¼Œä¼˜é›…åœæ­¢æ‰€æœ‰worker"""
        print(f"\n\nâš ï¸  æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å· ({signum})ï¼Œæ­£åœ¨åœæ­¢æ‰€æœ‰worker...")

        # è®¾ç½®åœæ­¢æ ‡å¿—ï¼Œé€šçŸ¥æ‰€æœ‰workeråœæ­¢
        if hasattr(self, "stop_flag"):
            self.stop_flag.set()

        # ä¿å­˜å½“å‰è¿›åº¦
        if hasattr(self, "current_progress") and self.checkpoint_file:
            self._save_checkpoint()
            print(f"âœ… è¿›åº¦å·²ä¿å­˜åˆ°: {self.checkpoint_file}")

        print("ğŸ”„ ç­‰å¾…workerä¼˜é›…é€€å‡º...")

        # ç»™workerä¸€äº›æ—¶é—´æ¥å®Œæˆå½“å‰ä»»åŠ¡
        import time

        time.sleep(2)

        print("ğŸ”„ å®‰å…¨é€€å‡º")
        sys.exit(0)

    def _save_checkpoint(self):
        """ä¿å­˜å½“å‰è¿›åº¦"""
        if not hasattr(self, "current_progress") or not self.checkpoint_file:
            return

        checkpoint_data = {
            "timestamp": datetime.now().isoformat(),
            "processed_count": len(self.current_progress),
            "progress_data": self.current_progress,
            "total_count": getattr(self, "total_count", 0),
            "sample_size": getattr(self, "sample_size", None),
        }

        with open(self.checkpoint_file, "w", encoding="utf-8") as f:
            json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)

    def _load_checkpoint(self, checkpoint_file: str) -> Dict:
        """åŠ è½½è¿›åº¦æ–‡ä»¶"""
        try:
            if os.path.exists(checkpoint_file):
                with open(checkpoint_file, "r", encoding="utf-8") as f:
                    return json.load(f)
        except Exception as e:
            print(f"è­¦å‘Š: æ— æ³•åŠ è½½è¿›åº¦æ–‡ä»¶ {checkpoint_file}: {e}")
        return {}

    def _call_ai_api(self, prompt: str, max_retries: int = 3) -> str:
        """è°ƒç”¨DeepSeek API"""
        for attempt in range(max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.1,
                    max_tokens=500,
                )

                # é‡ç½®å¤±è´¥è®¡æ•°
                self.consecutive_failures = 0
                return response.choices[0].message.content.strip()

            except Exception as e:
                self.consecutive_failures += 1
                if self.consecutive_failures == 1:  # åªåœ¨ç¬¬ä¸€æ¬¡å¤±è´¥æ—¶æ˜¾ç¤ºè¯¦ç»†é”™è¯¯
                    print(f"\nâŒ DeepSeek APIè°ƒç”¨å¤±è´¥: {e}")

                if self.consecutive_failures >= self.max_consecutive_failures:
                    print(
                        f"\nğŸš« è¿ç»­ {self.max_consecutive_failures} æ¬¡APIè°ƒç”¨å¤±è´¥ï¼Œåœæ­¢åˆ†æ"
                    )
                    print("è¯·æ£€æŸ¥ï¼š")
                    print("1. DEEPSEEK_API_KEY æ˜¯å¦æ­£ç¡®è®¾ç½®")
                    print("2. APIå¯†é’¥æ˜¯å¦æœ‰æ•ˆ")
                    print("3. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
                    print("4. DeepSeekæœåŠ¡æ˜¯å¦å¯ç”¨")
                    raise Exception("APIè°ƒç”¨è¿ç»­å¤±è´¥")

                if attempt < max_retries - 1:
                    time.sleep(2)

        return None

    def classify_single_review(self, review_text: str, is_positive: bool) -> List[str]:
        """
        å¯¹å•æ¡è¯„è®ºè¿›è¡ŒAIåˆ†ç±»

        Args:
            review_text: è¯„è®ºæ–‡æœ¬
            is_positive: æ˜¯å¦ä¸ºå¥½è¯„

        Returns:
            åˆ†ç±»ç»“æœåˆ—è¡¨
        """
        if not review_text:
            return []

        sentiment = "å¥½è¯„" if is_positive else "å·®è¯„"
        categories = self.categories["positive" if is_positive else "negative"]

        category_list = "\n".join(
            [f"- {name}: {desc}" for name, desc in categories.items()]
        )

        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„æ¸¸æˆè¯„è®ºåˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹ã€Šæ˜æœ«æ¸Šè™šä¹‹ç¾½ã€‹Steamè¯„è®ºï¼Œåˆ¤æ–­å®ƒå±äºå“ªäº›ç±»åˆ«ã€‚

è¯„è®ºå†…å®¹: "{review_text}"
è¯„è®ºç±»å‹: {sentiment}
è¯„è®ºé•¿åº¦: {'çŸ­è¯„' if len(review_text) <= 50 else 'é•¿è¯„' if len(review_text) >= 200 else 'ä¸­ç­‰é•¿åº¦'}

å¯é€‰ç±»åˆ«:
{category_list}

**é‡è¦è§„åˆ™**:
1. ä»”ç»†ç†è§£è¯„è®ºçš„è¯­ä¹‰å’Œæƒ…æ„Ÿå€¾å‘
2. æ³¨æ„è¯†åˆ«åè®½ã€é˜´é˜³æ€ªæ°”ç­‰è¡¨è¾¾æ–¹å¼
3. ä¸€æ¡è¯„è®ºå¯ä»¥å±äºå¤šä¸ªç±»åˆ«
4. åªè¿”å›é€‚ç”¨çš„ç±»åˆ«åç§°ï¼Œç”¨é€—å·åˆ†éš”
5. å¿…é¡»ä»ä¸Šè¿°ç±»åˆ«ä¸­é€‰æ‹©ï¼Œä¸è¦è‡ªåˆ›ç±»åˆ«
6. çŸ­è¯„ï¼ˆâ‰¤50å­—ï¼‰ï¼šåªé€‰1ä¸ªç±»åˆ«
7. **"å…¶ä»–"ç±»åˆ«è§„åˆ™**ï¼š
   - "å…¶ä»–" = æ²¡æœ‰å…·ä½“ç†ç”±çš„çº¯ç²¹è¡¨æ€ï¼ˆå¦‚"åƒåœ¾"ã€"ç¥ä½œ"ï¼‰
   - å¦‚æœé€‰æ‹©"å…¶ä»–"ï¼Œå°±åªèƒ½æ˜¯"å…¶ä»–"ï¼Œä¸èƒ½æœ‰ä»»ä½•å…¶ä»–ç±»åˆ«
   - å¦‚æœæœ‰å…·ä½“é—®é¢˜/ä¼˜ç‚¹ï¼Œå°±ä¸èƒ½é€‰"å…¶ä»–"
8. é•¿è¯„ï¼šå¯ä»¥å¤šç±»åˆ«ï¼Œä½†å¿…é¡»çœŸçš„æ¶‰åŠå¤šä¸ªæ–¹é¢

è¾“å‡ºè¦æ±‚ï¼š
- åªè¾“å‡ºç±»åˆ«åç§°ï¼Œç”¨é€—å·åˆ†éš”
- ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šã€ç†ç”±ã€JSONã€æ€è€ƒè¿‡ç¨‹æˆ–åŒ…å«<think>æ ‡ç­¾çš„å†…å®¹
- å¦‚æœæ²¡æœ‰æ˜ç¡®ç±»åˆ«ï¼Œè¿”å›"æ— æ˜ç¡®ç±»åˆ«"

æ­£ç¡®ç¤ºä¾‹:
- "å…¶ä»–"
- "æ¸¸æˆè´¨é‡"
- "å‰§æƒ…æ•…äº‹,ç¾æœ¯éŸ³æ•ˆ"

è¾“å‡º:"""

        ai_response = self._call_ai_api(prompt)

        if not ai_response:
            print(f"âŒ APIè¿”å›ç©ºå“åº”ï¼Œè¿™æ˜¯æŠ€æœ¯é—®é¢˜ï¼Œåœæ­¢åˆ†æ: {review_text[:50]}...")
            raise Exception(f"APIè°ƒç”¨å¤±è´¥ï¼šè¿”å›ç©ºå“åº”ã€‚è¯„è®º: {review_text[:50]}...")

        # æ¸…ç†æ¨¡å‹å¯èƒ½è¿”å›çš„æ€è€ƒå†…å®¹å’Œæ ‡ç­¾ï¼Œé¿å…å½±å“è§£æ
        try:
            import re

            # ç§»é™¤<think>...</think>å†…å®¹
            ai_response = re.sub(
                r"<think>[\s\S]*?</think>", "", ai_response, flags=re.IGNORECASE
            )
            # å»æ‰ä»»ä½•æ®‹ä½™çš„å°–æ‹¬å·æ ‡ç­¾
            ai_response = re.sub(r"<[^>]+>", "", ai_response)
            ai_response = ai_response.strip()
        except Exception:
            pass

        # è§£æAIå“åº”
        result_categories = []
        raw_categories = ai_response.replace("ã€", ",").replace("ï¼Œ", ",").split(",")

        for cat in raw_categories:
            cat = cat.strip()
            # ç§»é™¤å¯èƒ½çš„æ ‡ç‚¹ç¬¦å·
            cat = cat.rstrip("ã€‚ï¼Œ,.")

            if cat and cat != "æ— æ˜ç¡®ç±»åˆ«":
                # ä¸¥æ ¼åŒ¹é…
                if cat in categories:
                    result_categories.append(cat)
                else:
                    # å¦‚æœä¸¥æ ¼åŒ¹é…å¤±è´¥ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
                    found = False
                    for valid_cat in categories:
                        if cat.lower() == valid_cat.lower():  # å¤§å°å†™ä¸æ•æ„Ÿ
                            result_categories.append(valid_cat)
                            found = True
                            break

                    if not found:
                        print(
                            f"âš ï¸  æ— æ³•è¯†åˆ«çš„ç±»åˆ«: '{cat}' (åŸå§‹å›å¤: {ai_response[:100]}...)"
                        )

        # å¼ºåˆ¶æ‰§è¡Œ"å…¶ä»–"ç±»åˆ«çš„æ’ä»–æ€§
        if "å…¶ä»–" in result_categories and len(result_categories) > 1:
            # å¦‚æœåŒæ—¶åŒ…å«"å…¶ä»–"å’Œå…¶ä»–ç±»åˆ«ï¼Œä¼˜å…ˆé€‰æ‹©å…·ä½“ç±»åˆ«
            result_categories = [cat for cat in result_categories if cat != "å…¶ä»–"]
            print(f"âš ï¸  AIè¿åæ’ä»–æ€§è§„åˆ™ï¼Œè‡ªåŠ¨ä¿®æ­£: ç§»é™¤'å…¶ä»–'ï¼Œä¿ç•™{result_categories}")

        # é˜²ç©ºåˆ†ç±»ï¼šå¦‚æœè§£æåæ— æœ‰æ•ˆç±»åˆ«ï¼Œè‡ªåŠ¨åˆ†é…åˆ°"å…¶ä»–"ç±»åˆ«
        # è¿™é€šå¸¸æ˜¯å› ä¸ºæ¨¡å‹è¿”å›äº†æ— æ³•è¯†åˆ«çš„ç±»åˆ«åç§°ï¼Œå±äºç†è§£é—®é¢˜è€ŒéæŠ€æœ¯é—®é¢˜
        if not result_categories:
            result_categories = ["å…¶ä»–"]
            print(f"âš ï¸  AIè¿”å›æ— æ³•è¯†åˆ«çš„ç±»åˆ«ï¼Œå½’ç±»ä¸º'å…¶ä»–': {review_text[:50]}...")

        return result_categories

    def _insert_result_in_order(self, result):
        """æŒ‰indexé¡ºåºæ’å…¥ç»“æœï¼Œä¿æŒprogress_dataæœ‰åº"""
        index = result["index"]

        # äºŒåˆ†æŸ¥æ‰¾æ’å…¥ä½ç½®
        left, right = 0, len(self.current_progress)
        while left < right:
            mid = (left + right) // 2
            if self.current_progress[mid]["index"] < index:
                left = mid + 1
            else:
                right = mid

        # åœ¨æ­£ç¡®ä½ç½®æ’å…¥
        with self.progress_lock:
            self.current_progress.insert(left, result)

    def _create_worker_analyzer(self) -> "ReviewAnalyzer":
        """ä¸ºæ¯ä¸ªworkeråˆ›å»ºç‹¬ç«‹çš„ReviewAnalyzerå®ä¾‹"""
        # åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„workerå®ä¾‹ï¼Œä¸è®¾ç½®signalå¤„ç†å™¨
        worker = object.__new__(ReviewAnalyzer)  # ä¸è°ƒç”¨__init__
        worker.api_key = self.api_key
        worker.client = OpenAI(
            api_key=self.api_key, base_url="https://api.deepseek.com"
        )
        worker.model = "deepseek-chat"
        worker.categories = self.categories
        worker.consecutive_failures = 0
        worker.max_consecutive_failures = 3
        return worker

    def _parallel_worker(
        self,
        task_queue: queue.Queue,
        results_queue: queue.Queue,
        reviews_df: pd.DataFrame,
        worker_id: int,
    ):
        """å¹¶è¡Œworkerå‡½æ•°ï¼Œå¤„ç†ä»»åŠ¡é˜Ÿåˆ—ä¸­çš„è¯„è®º"""
        # ä¸ºè¿™ä¸ªworkeråˆ›å»ºç‹¬ç«‹çš„APIå®¢æˆ·ç«¯
        worker_analyzer = self._create_worker_analyzer()
        processed_count = 0

        print(f"ğŸš€ Worker {worker_id} å¯åŠ¨")

        while True:
            # æ£€æŸ¥åœæ­¢æ ‡å¿—
            if self.stop_flag.is_set():
                print(f"ğŸ›‘ Worker {worker_id} æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
                break

            try:
                # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡ï¼Œ1ç§’è¶…æ—¶
                task = task_queue.get(timeout=1)
                if task is None:  # åœæ­¢ä¿¡å·
                    break

                idx, df_idx = task
                row = reviews_df.loc[df_idx]
                review_text = str(row.get("review_text", ""))
                is_positive = bool(row.get("voted_up", True))

                try:
                    # å¤„ç†å•æ¡è¯„è®º
                    categories = worker_analyzer.classify_single_review(
                        review_text, is_positive
                    )

                    # æ„å»ºç»“æœ
                    result = {
                        "index": df_idx,
                        "categories": categories,
                        "is_positive": is_positive,
                    }

                    # å°†ç»“æœæ”¾å…¥ç»“æœé˜Ÿåˆ—
                    results_queue.put(result)
                    processed_count += 1

                    # æ§åˆ¶è¯·æ±‚é¢‘ç‡
                    time.sleep(self.request_delay)

                except Exception as e:
                    print(f"âš ï¸ Worker {worker_id} å¤„ç†è¯„è®º {idx} å¤±è´¥: {e}")
                    # å³ä½¿å¤±è´¥ä¹Ÿè¦è¿”å›ä¸€ä¸ªç»“æœï¼Œé¿å…ä¸¢å¤±è¿›åº¦
                    result = {
                        "index": df_idx,
                        "categories": [],
                        "is_positive": is_positive,
                        "error": str(e),
                    }
                    results_queue.put(result)

                # æ ‡è®°ä»»åŠ¡å®Œæˆ
                task_queue.task_done()

            except queue.Empty:
                # é˜Ÿåˆ—ä¸ºç©ºï¼Œç»§ç»­ç­‰å¾…
                continue
            except Exception as e:
                print(f"âŒ Worker {worker_id} å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
                break

        print(f"âœ… Worker {worker_id} å®Œæˆï¼Œå¤„ç†äº† {processed_count} æ¡è¯„è®º")

    def classify_batch_parallel(
        self,
        reviews_df: pd.DataFrame,
        sample_size: int = None,
        output_dir: str = "analysis_results",
    ) -> pd.DataFrame:
        """
        å¹¶è¡Œæ‰¹é‡åˆ†ç±»è¯„è®ºï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰

        Args:
            reviews_df: è¯„è®ºæ•°æ®
            sample_size: æ ·æœ¬å¤§å°ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨å¤„ç†
            output_dir: è¾“å‡ºç›®å½•ï¼Œç”¨äºç”Ÿæˆcheckpointæ–‡ä»¶å

        Returns:
            æ·»åŠ äº†åˆ†ç±»ç»“æœçš„DataFrame
        """
        print("ğŸš€ å¼€å§‹å¹¶è¡ŒAIåˆ†ç±»åˆ†æ...")
        print(
            f"ğŸ”§ é…ç½®: {self.parallel_workers}ä¸ªå¹¶è¡Œworkerï¼Œè¯·æ±‚é—´éš”{self.request_delay}ç§’"
        )

        # é€‰æ‹©å¤„ç†èŒƒå›´
        if sample_size and sample_size < len(reviews_df):
            df_to_process = reviews_df.sample(n=sample_size, random_state=42).copy()
            print(f"éšæœºæŠ½æ · {sample_size} æ¡è¯„è®ºè¿›è¡Œåˆ†æï¼ˆå…± {len(reviews_df)} æ¡ï¼‰")
        else:
            df_to_process = reviews_df.copy()
            print(f"å¤„ç†å…¨éƒ¨ {len(df_to_process)} æ¡è¯„è®º")

        # è®¾ç½®checkpointæ–‡ä»¶
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        self.checkpoint_file = os.path.join(output_dir, "classification_progress.json")

        # å­˜å‚¨è¿›åº¦ç›¸å…³å˜é‡
        self.total_count = len(df_to_process)
        self.sample_size = sample_size

        # åŠ è½½å·²æœ‰è¿›åº¦
        checkpoint_data = self._load_checkpoint(self.checkpoint_file)
        start_idx = 0

        if checkpoint_data and checkpoint_data.get("progress_data"):
            self.current_progress = checkpoint_data["progress_data"]
            start_idx = len(self.current_progress)
            print(
                f"ğŸ”„ å‘ç°æ–­ç‚¹æ–‡ä»¶ï¼Œå·²å¤„ç† {start_idx} æ¡ï¼Œä»ç¬¬ {start_idx + 1} æ¡å¼€å§‹å¹¶è¡Œå¤„ç†"
            )
        else:
            self.current_progress = []

        total_reviews = len(df_to_process)
        remaining_reviews = total_reviews - start_idx

        if remaining_reviews <= 0:
            print("âœ… æ‰€æœ‰è¯„è®ºå·²å¤„ç†å®Œæˆï¼")
            return self._rebuild_dataframe_from_progress(df_to_process)

        print(f"ğŸ“Š å‰©ä½™ {remaining_reviews} æ¡è¯„è®ºéœ€è¦å¤„ç†")

        # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—å’Œç»“æœé˜Ÿåˆ—
        task_queue = queue.Queue()
        results_queue = queue.Queue()

        # å°†å‰©ä½™ä»»åŠ¡æ”¾å…¥é˜Ÿåˆ—
        for idx in range(start_idx, total_reviews):
            df_idx = df_to_process.iloc[idx].name
            task_queue.put((idx, df_idx))

        # å¯åŠ¨å¹¶è¡Œworkers
        start_time = time.time()
        workers = []

        for worker_id in range(self.parallel_workers):
            worker = threading.Thread(
                target=self._parallel_worker,
                args=(task_queue, results_queue, df_to_process, worker_id),
            )
            worker.start()
            workers.append(worker)

        # ç›‘æ§è¿›åº¦å¹¶æ”¶é›†ç»“æœ
        completed_count = 0
        last_save_time = time.time()

        while completed_count < remaining_reviews:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
            if self.stop_flag.is_set():
                print("ğŸ›‘ ä¸»çº¿ç¨‹æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨ç»ˆæ­¢å¤„ç†...")
                break

            try:
                # ä»ç»“æœé˜Ÿåˆ—è·å–ç»“æœ
                result = results_queue.get(timeout=2)
                self._insert_result_in_order(result)
                completed_count += 1

                # æ˜¾ç¤ºè¿›åº¦
                if completed_count % 10 == 0 or completed_count == remaining_reviews:
                    elapsed = time.time() - start_time
                    if completed_count > 0:
                        estimated_total = elapsed * remaining_reviews / completed_count
                        remaining_time = estimated_total - elapsed
                    else:
                        remaining_time = 0

                    current_total = start_idx + completed_count
                    progress_pct = current_total / total_reviews * 100

                    print(
                        f"ğŸ”„ å¹¶è¡Œå¤„ç†è¿›åº¦: {current_total}/{total_reviews} ({progress_pct:.1f}%) "
                        f"é¢„è®¡å‰©ä½™: {remaining_time/60:.1f}åˆ†é’Ÿ"
                    )

                # å®šæœŸä¿å­˜è¿›åº¦ï¼ˆæ¯30ç§’æˆ–æ¯50æ¡ï¼‰
                current_time = time.time()
                if (
                    completed_count % 50 == 0
                    or current_time - last_save_time > 30
                    or completed_count == remaining_reviews
                ):

                    self._save_checkpoint()
                    last_save_time = current_time

            except queue.Empty:
                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰workeréƒ½å®Œæˆäº†
                alive_workers = sum(1 for w in workers if w.is_alive())
                if alive_workers == 0:
                    print("âš ï¸ æ‰€æœ‰workerå·²å®Œæˆï¼Œä½†å¯èƒ½æœ‰æœªå¤„ç†çš„ä»»åŠ¡")
                    break
                continue

        # ç­‰å¾…æ‰€æœ‰workerå®Œæˆ
        print("ğŸ”„ ç­‰å¾…æ‰€æœ‰workerå®Œæˆ...")
        for i, worker in enumerate(workers):
            worker.join(timeout=5)  # å‡å°‘è¶…æ—¶æ—¶é—´åˆ°5ç§’
            if worker.is_alive():
                print(f"âš ï¸ Worker {i+1} ä»åœ¨è¿è¡Œï¼Œå¼ºåˆ¶ç»§ç»­...")

        # æœ€ç»ˆä¿å­˜
        self._save_checkpoint()

        print(
            f"âœ… å¹¶è¡Œåˆ†æå®Œæˆï¼å…±å¤„ç† {len(self.current_progress)} æ¡è¯„è®ºï¼Œ"
            f"è€—æ—¶ {(time.time() - start_time)/60:.1f} åˆ†é’Ÿ"
        )

        # ä¿ç•™è¿›åº¦æ–‡ä»¶ï¼Œä¸åˆ é™¤ç”¨æˆ·æ•°æ®
        print(f"ğŸ“„ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {self.checkpoint_file}")
        print("ğŸ’¡ å¦‚éœ€é‡æ–°åˆ†æï¼Œè¯·æ‰‹åŠ¨åˆ é™¤è¿›åº¦æ–‡ä»¶")
        print("")
        print("ğŸ¯ ä¸‹ä¸€æ­¥: ä½¿ç”¨ report_generator.py ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š")

        return self._rebuild_dataframe_from_progress(df_to_process)

    def _rebuild_dataframe_from_progress(
        self, df_to_process: pd.DataFrame
    ) -> pd.DataFrame:
        """ä»è¿›åº¦æ•°æ®é‡å»ºDataFrame"""
        # å°†ç»“æœåº”ç”¨åˆ°DataFrame
        ai_categories = [[] for _ in range(len(df_to_process))]

        for result in self.current_progress:
            df_idx = result["index"]
            categories = result["categories"]

            try:
                position = df_to_process.index.get_loc(df_idx)
                ai_categories[position] = categories
            except KeyError:
                continue

        df_to_process["ai_categories"] = ai_categories
        return df_to_process

    def classify_batch(
        self,
        reviews_df: pd.DataFrame,
        sample_size: int = None,
        output_dir: str = "ai_analysis",
    ) -> pd.DataFrame:
        """
        æ‰¹é‡åˆ†ç±»è¯„è®ºï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰

        Args:
            reviews_df: è¯„è®ºæ•°æ®
            sample_size: æ ·æœ¬å¤§å°ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨å¤„ç†
            output_dir: è¾“å‡ºç›®å½•ï¼Œç”¨äºç”Ÿæˆcheckpointæ–‡ä»¶å

        Returns:
            æ·»åŠ äº†åˆ†ç±»ç»“æœçš„DataFrame
        """
        print("å¼€å§‹çº¯AIåˆ†ç±»åˆ†æ...")

        # é€‰æ‹©å¤„ç†èŒƒå›´
        if sample_size and sample_size < len(reviews_df):
            df_to_process = reviews_df.sample(n=sample_size, random_state=42).copy()
            print(f"éšæœºæŠ½æ · {sample_size} æ¡è¯„è®ºè¿›è¡Œåˆ†æï¼ˆå…± {len(reviews_df)} æ¡ï¼‰")
        else:
            df_to_process = reviews_df.copy()
            print(f"å¤„ç†å…¨éƒ¨ {len(df_to_process)} æ¡è¯„è®º")

        # è®¾ç½®checkpointæ–‡ä»¶
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        self.checkpoint_file = os.path.join(output_dir, "classification_progress.json")

        # å­˜å‚¨è¿›åº¦ç›¸å…³å˜é‡
        self.total_count = len(df_to_process)
        self.sample_size = sample_size

        # å…ˆæ£€æŸ¥æ˜¯å¦æœ‰æ–­ç‚¹æ–‡ä»¶ï¼Œå†åˆå§‹åŒ–è¿›åº¦
        checkpoint_data = self._load_checkpoint(self.checkpoint_file)
        start_idx = 0

        # æ ¹æ®checkpointæ•°æ®åˆå§‹åŒ–current_progress
        if checkpoint_data and checkpoint_data.get("progress_data"):
            self.current_progress = checkpoint_data["progress_data"]
        else:
            self.current_progress = []

        # å¦‚æœæœ‰checkpointæ•°æ®ï¼Œè¯¢é—®ç”¨æˆ·æ“ä½œ
        if len(self.current_progress) > 0:
            print(f"ğŸ”„ å‘ç°æ–­ç‚¹æ–‡ä»¶ï¼Œå·²å¤„ç† {len(self.current_progress)} æ¡")
            print("é€‰æ‹©æ“ä½œï¼š")
            print("1. ç»§ç»­AIåˆ†æ (C)")
            print("2. åŸºäºç°æœ‰æ•°æ®ç”ŸæˆæŠ¥å‘Š (R)")
            choice = input("è¯·é€‰æ‹© (C/r): ").lower().strip()

            if choice == "r":
                print("ğŸ¯ åŸºäºç°æœ‰æ•°æ®ç”ŸæˆæŠ¥å‘Š...")
                # æ„å»ºç®€åŒ–çš„DataFrameç”¨äºæŠ¥å‘Šç”Ÿæˆ
                classified_data = []
                for item in self.current_progress:
                    row = {
                        "index": item["index"],
                        "ai_categories": item["categories"],
                        "voted_up": item.get("is_positive", True),
                        "analysis_is_positive": item.get("is_positive", True),
                        "review_text": f"è¯„è®º{item['index']}",
                        "votes_up": 0,
                        "author_playtime_hours": 0,
                    }
                    if item["index"] < len(df_to_process):
                        original_row = df_to_process.iloc[item["index"]]
                        row.update(
                            {
                                "review_text": original_row.get(
                                    "review_text", f"è¯„è®º{item['index']}"
                                ),
                                "votes_up": original_row.get("votes_up", 0),
                                "author_playtime_hours": original_row.get(
                                    "author_playtime_hours", 0
                                ),
                            }
                        )
                    classified_data.append(row)

                return pd.DataFrame(classified_data)
            elif choice == "c" or choice == "":
                start_idx = len(self.current_progress)
                print(f"âœ… ä»ç¬¬ {start_idx + 1} æ¡å¼€å§‹ç»§ç»­åˆ†æ")
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œé€€å‡ºç¨‹åº")
                return None

        total_reviews = len(df_to_process)
        start_time = time.time()

        if start_idx > 0:
            print(
                f"ğŸ“Š ç»­ä¼ è¿›åº¦: å·²å®Œæˆ {start_idx}/{total_reviews} ({start_idx/total_reviews*100:.1f}%)"
            )

        # å¤„ç†è¯„è®ºï¼ˆä»æ–­ç‚¹å¼€å§‹ï¼‰
        for idx, (df_idx, row) in enumerate(df_to_process.iterrows()):
            if idx < start_idx:
                continue

            if idx % 10 == 0 and idx >= start_idx:
                elapsed = time.time() - start_time
                if idx > start_idx:
                    estimated_total = (elapsed / (idx - start_idx)) * (
                        total_reviews - start_idx
                    )
                    remaining = estimated_total - elapsed
                else:
                    remaining = 0

                print(
                    f"AIåˆ†æè¿›åº¦: {idx + 1}/{total_reviews} ({(idx + 1)/total_reviews*100:.1f}%) "
                    f"é¢„è®¡å‰©ä½™: {remaining/60:.1f}åˆ†é’Ÿ"
                )

            review_text = str(row.get("review_text", ""))
            is_positive = bool(row.get("voted_up", True))

            try:
                categories = self.classify_single_review(review_text, is_positive)
            except Exception as e:
                if "APIè°ƒç”¨è¿ç»­å¤±è´¥" in str(e):
                    print(f"\nğŸ’¾ å·²ä¿å­˜å‰ {idx} æ¡è¯„è®ºçš„åˆ†æç»“æœ")
                    self._save_checkpoint()
                    raise e
                else:
                    print(f"è­¦å‘Š: ç¬¬{idx + 1}æ¡è¯„è®ºåˆ†æå¤±è´¥: {e}")
                    categories = []

            review_result = {
                "index": df_idx,
                "categories": categories,
                "is_positive": is_positive,
            }
            self.current_progress.append(review_result)

            # å®šæœŸä¿å­˜è¿›åº¦
            if (idx + 1) % self.auto_save_interval == 0:
                self._save_checkpoint()

            # æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œé¿å…è§¦å‘APIé™åˆ¶
            time.sleep(0.2)

        # æœ€ç»ˆä¿å­˜
        self._save_checkpoint()

        # å°†ç»“æœåº”ç”¨åˆ°DataFrame
        ai_categories = [[] for _ in range(len(df_to_process))]

        for result in self.current_progress:
            df_idx = result["index"]
            categories = result["categories"]
            result_is_positive = result.get("is_positive", True)  # å…¼å®¹æ—§æ ¼å¼

            try:
                position = df_to_process.index.get_loc(df_idx)
                # éªŒè¯åˆ†ç±»æ˜¯å¦ä¸æƒ…æ„Ÿå€¾å‘åŒ¹é…
                actual_is_positive = bool(df_to_process.iloc[position]["voted_up"])

                if result_is_positive == actual_is_positive:
                    ai_categories[position] = categories
                else:
                    # å¦‚æœæƒ…æ„Ÿå€¾å‘ä¸åŒ¹é…ï¼Œé‡æ–°åˆ†ç±»è¿™æ¡è¯„è®º
                    print(f"è­¦å‘Š: ç¬¬{position + 1}æ¡è¯„è®ºæƒ…æ„Ÿå€¾å‘ä¸åŒ¹é…ï¼Œè·³è¿‡")
                    ai_categories[position] = []

            except KeyError:
                continue

        df_to_process["ai_categories"] = ai_categories

        print(
            f"AIåˆ†ç±»å®Œæˆï¼å¤„ç†äº† {len(self.current_progress)} æ¡è¯„è®ºï¼Œè€—æ—¶ {(time.time() - start_time)/60:.1f} åˆ†é’Ÿ"
        )

        # ä¿ç•™è¿›åº¦æ–‡ä»¶ï¼Œä¸åˆ é™¤ç”¨æˆ·æ•°æ®
        print(f"ğŸ“„ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {self.checkpoint_file}")
        print("ğŸ’¡ å¦‚éœ€é‡æ–°åˆ†æï¼Œè¯·æ‰‹åŠ¨åˆ é™¤è¿›åº¦æ–‡ä»¶")

        return df_to_process

    def generate_statistics(self, classified_df: pd.DataFrame) -> Dict:
        """ç”Ÿæˆç»Ÿè®¡æ•°æ®"""
        stats = {
            "total_reviews": len(classified_df),
            "positive_reviews": len(classified_df[classified_df["voted_up"] == True]),
            "negative_reviews": len(classified_df[classified_df["voted_up"] == False]),
            "positive_categories": {},
            "negative_categories": {},
            "multi_category_stats": {},
            "uncategorized": 0,
        }

        # åˆ†åˆ«ç»Ÿè®¡å¥½è¯„å’Œå·®è¯„
        positive_df = classified_df[classified_df["voted_up"] == True]
        negative_df = classified_df[classified_df["voted_up"] == False]

        # ç»Ÿè®¡å¥½è¯„ç±»åˆ«
        all_positive_cats = []
        for categories in positive_df["ai_categories"]:
            if categories:
                all_positive_cats.extend(categories)

        positive_counter = Counter(all_positive_cats)
        for cat_name, count in positive_counter.items():
            stats["positive_categories"][cat_name] = {
                "count": count,
                "percentage": (
                    count / stats["positive_reviews"] * 100
                    if stats["positive_reviews"] > 0
                    else 0
                ),
            }

        # ç»Ÿè®¡å·®è¯„ç±»åˆ«
        all_negative_cats = []
        for categories in negative_df["ai_categories"]:
            if categories:
                all_negative_cats.extend(categories)

        negative_counter = Counter(all_negative_cats)
        for cat_name, count in negative_counter.items():
            stats["negative_categories"][cat_name] = {
                "count": count,
                "percentage": (
                    count / stats["negative_reviews"] * 100
                    if stats["negative_reviews"] > 0
                    else 0
                ),
            }

        # å¤šç±»åˆ«ç»Ÿè®¡
        multi_cat_counts = defaultdict(int)
        uncategorized_count = 0

        for _, row in classified_df.iterrows():
            cat_count = len(row["ai_categories"])
            if cat_count == 0:
                uncategorized_count += 1
            elif cat_count > 1:
                multi_cat_counts[cat_count] += 1

        stats["multi_category_stats"] = dict(multi_cat_counts)
        stats["uncategorized"] = uncategorized_count

        return stats

    def get_representative_reviews(
        self, classified_df: pd.DataFrame, max_per_category: int = None
    ) -> Dict:
        """è·å–æ¯ä¸ªç±»åˆ«çš„ä»£è¡¨æ€§è¯„è®º"""
        if max_per_category is None:
            max_per_category = int(os.getenv("MAX_REPRESENTATIVE_REVIEWS", "5"))

        representative = {}

        # æ”¶é›†æ‰€æœ‰ç±»åˆ«
        all_categories = set()
        for categories in classified_df["ai_categories"]:
            all_categories.update(categories)

        for category_name in all_categories:
            category_reviews = []

            for idx, row in classified_df.iterrows():
                if category_name in row["ai_categories"]:
                    category_reviews.append(
                        {
                            "review_text": row["review_text"],
                            "votes_up": row.get("votes_up", 0),
                            "voted_up": row.get("voted_up", True),
                            "created_date": row.get("created_date", ""),
                            "author_playtime_hours": row.get(
                                "author_playtime_hours", 0
                            ),
                        }
                    )

            # æŒ‰ç‚¹èµæ•°æ’åº
            category_reviews.sort(key=lambda x: x["votes_up"], reverse=True)
            representative[category_name] = category_reviews[:max_per_category]

        return representative

    def create_visualizations(self, stats: Dict, output_dir: str):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        plt.rcParams["font.sans-serif"] = ["SimHei", "DejaVu Sans"]
        plt.rcParams["axes.unicode_minus"] = False

        # 1. å¥½è¯„ç±»åˆ«åˆ†å¸ƒ
        if stats["positive_categories"]:
            plt.figure(figsize=(12, 8))
            labels = list(stats["positive_categories"].keys())
            counts = [cat["count"] for cat in stats["positive_categories"].values()]

            bars = plt.bar(range(len(labels)), counts, color="#2ecc71", alpha=0.8)

            for bar, count in zip(bars, counts):
                height = bar.get_height()
                plt.text(
                    bar.get_x() + bar.get_width() / 2.0,
                    height + height * 0.01,
                    f"{count}",
                    ha="center",
                    va="bottom",
                    fontsize=11,
                )

            plt.title("å¥½è¯„ç±»åˆ«åˆ†å¸ƒ (AIåˆ†æ)", fontsize=16, fontweight="bold")
            plt.xlabel("ç±»åˆ«", fontsize=12)
            plt.ylabel("è¯„è®ºæ•°é‡", fontsize=12)
            plt.xticks(range(len(labels)), labels, rotation=45, ha="right")
            plt.tight_layout()
            plt.savefig(
                f"{output_dir}/positive_categories_ai.png", dpi=300, bbox_inches="tight"
            )
            plt.close()

        # 2. å·®è¯„ç±»åˆ«åˆ†å¸ƒ
        if stats["negative_categories"]:
            plt.figure(figsize=(12, 8))
            labels = list(stats["negative_categories"].keys())
            counts = [cat["count"] for cat in stats["negative_categories"].values()]

            bars = plt.bar(range(len(labels)), counts, color="#e74c3c", alpha=0.8)

            for bar, count in zip(bars, counts):
                height = bar.get_height()
                plt.text(
                    bar.get_x() + bar.get_width() / 2.0,
                    height + height * 0.01,
                    f"{count}",
                    ha="center",
                    va="bottom",
                    fontsize=11,
                )

            plt.title("å·®è¯„ç±»åˆ«åˆ†å¸ƒ (AIåˆ†æ)", fontsize=16, fontweight="bold")
            plt.xlabel("ç±»åˆ«", fontsize=12)
            plt.ylabel("è¯„è®ºæ•°é‡", fontsize=12)
            plt.xticks(range(len(labels)), labels, rotation=45, ha="right")
            plt.tight_layout()
            plt.savefig(
                f"{output_dir}/negative_categories_ai.png", dpi=300, bbox_inches="tight"
            )
            plt.close()

        # 3. æ€»ä½“å¥½å·®è¯„æ¯”ä¾‹
        plt.figure(figsize=(8, 6))
        labels = ["å¥½è¯„", "å·®è¯„"]
        sizes = [stats["positive_reviews"], stats["negative_reviews"]]
        colors = ["#2ecc71", "#e74c3c"]

        plt.pie(sizes, labels=labels, autopct="%1.1f%%", colors=colors, startangle=90)
        plt.title("å¥½è¯„å·®è¯„æ¯”ä¾‹", fontsize=16, fontweight="bold")
        plt.savefig(
            f"{output_dir}/overall_sentiment_ai.png", dpi=300, bbox_inches="tight"
        )
        plt.close()

        print(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ° {output_dir} ç›®å½•")

    def generate_report(
        self, classified_df: pd.DataFrame, output_dir: str = "ai_analysis"
    ) -> str:
        """ç”Ÿæˆå®Œæ•´çš„AIåˆ†ææŠ¥å‘Š"""
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        # ç”Ÿæˆç»Ÿè®¡æ•°æ®
        stats = self.generate_statistics(classified_df)

        # è·å–ä»£è¡¨æ€§è¯„è®º
        representative = self.get_representative_reviews(classified_df)

        # åˆ›å»ºå¯è§†åŒ–
        self.create_visualizations(stats, output_dir)

        # ç”ŸæˆHTMLæŠ¥å‘Š
        report_path = f"{output_dir}/ai_analysis_report.html"
        html_content = self._generate_html_report(stats, representative)

        with open(report_path, "w", encoding="utf-8") as f:
            f.write(html_content)

        # ä¿å­˜æ•°æ®
        json_path = f"{output_dir}/ai_analysis_data.json"
        analysis_data = {
            "statistics": stats,
            "representative_reviews": representative,
            "generation_time": datetime.now().isoformat(),
        }

        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(analysis_data, f, ensure_ascii=False, indent=2)

        # ä¿å­˜åˆ†ç±»ç»“æœ
        csv_path = f"{output_dir}/ai_classified_reviews.csv"
        classified_df.to_csv(csv_path, index=False, encoding="utf-8-sig")

        print(f"\nAIåˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ:")
        print(f"  HTMLæŠ¥å‘Š: {report_path}")
        print(f"  æ•°æ®æ–‡ä»¶: {json_path}")
        print(f"  åˆ†ç±»ç»“æœ: {csv_path}")

        return report_path

    def _generate_html_report(self, stats: Dict, representative: Dict) -> str:
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        html = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <title>AIè¯„è®ºåˆ†ææŠ¥å‘Š - æ˜æœ«æ¸Šè™šä¹‹ç¾½</title>
    <style>
        body {{ font-family: -apple-system, BlinkMacSystemFont, sans-serif; margin: 40px; line-height: 1.6; }}
        .header {{ text-align: center; margin-bottom: 40px; }}
        .summary {{ background: #f8f9fa; padding: 20px; border-radius: 8px; margin-bottom: 30px; }}
        .category-section {{ margin-bottom: 40px; }}
        .category-title {{ color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; }}
        .category-item {{ background: white; border-left: 4px solid #3498db; padding: 15px; margin: 10px 0; border-radius: 4px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
        .positive {{ border-left-color: #27ae60; }}
        .negative {{ border-left-color: #e74c3c; }}
        .review-text {{ background: #f8f9fa; padding: 10px; border-radius: 4px; margin: 10px 0; font-style: italic; }}
        .stats {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin: 20px 0; }}
        .stat-card {{ background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); text-align: center; }}
        .stat-number {{ font-size: 2em; font-weight: bold; color: #3498db; }}
        .representative-review {{ margin: 15px 0; padding: 15px; background: #f8f9fa; border-radius: 6px; }}
        .review-meta {{ color: #666; font-size: 0.9em; margin-top: 10px; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ã€Šæ˜æœ«æ¸Šè™šä¹‹ç¾½ã€‹Steamè¯„è®ºAIåˆ†ææŠ¥å‘Š</h1>
        <p>åŸºäºDeepSeek AIæ·±åº¦è¯­ä¹‰åˆ†æ</p>
        <p>ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    </div>
    
    <div class="summary">
        <h2>æ€»ä½“æ¦‚å†µ</h2>
        <div class="stats">
            <div class="stat-card">
                <div class="stat-number">{stats['total_reviews']}</div>
                <div>æ€»åˆ†ææ•°é‡</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{stats['positive_reviews']}</div>
                <div>å¥½è¯„æ•°é‡</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{stats['negative_reviews']}</div>
                <div>å·®è¯„æ•°é‡</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{stats['positive_reviews']/(stats['total_reviews'])*100:.1f}%</div>
                <div>å¥½è¯„ç‡</div>
            </div>
            <div class="stat-card">
                <div class="stat-number">{stats['uncategorized']}</div>
                <div>æ— æ³•åˆ†ç±»</div>
            </div>
        </div>
    </div>
    
    <div class="category-section">
        <h2 class="category-title positive">å¥½è¯„ç±»åˆ«åˆ†æ</h2>
        <p><em>AIæ·±åº¦è¯­ä¹‰åˆ†æï¼Œå‡†ç¡®ç†è§£è¯„è®ºæ„å›¾</em></p>
        """

        # å¥½è¯„ç±»åˆ«
        for cat_name, cat_data in stats["positive_categories"].items():
            display_name = f"{cat_name}ï¼ˆå¥½è¯„ï¼‰" if cat_name == "å…¶ä»–" else cat_name
            html += f"""
        <div class="category-item positive">
            <h3>{display_name}</h3>
            <p><strong>{cat_data['count']}</strong> æ¡è¯„è®º ({cat_data['percentage']:.1f}%)</p>
            """

            if cat_name in representative:
                html += "<h4>ä»£è¡¨æ€§è¯„è®º:</h4>"
                for review in representative[cat_name][:3]:
                    html += f"""
                    <div class="representative-review">
                        <div class="review-text">"{review['review_text'][:200]}{'...' if len(review['review_text']) > 200 else ''}"</div>
                        <div class="review-meta">
                            ğŸ‘ {review['votes_up']} ç‚¹èµ | 
                            â±ï¸ æ¸¸æˆæ—¶é•¿: {review.get('author_playtime_hours', 0):.1f}å°æ—¶
                        </div>
                    </div>
                    """

            html += "</div>"

        html += """
    </div>
    
    <div class="category-section">
        <h2 class="category-title negative">å·®è¯„ç±»åˆ«åˆ†æ</h2>
        <p><em>AIæ·±åº¦è¯­ä¹‰åˆ†æï¼Œå‡†ç¡®ç†è§£è¯„è®ºæ„å›¾</em></p>
        """

        # å·®è¯„ç±»åˆ«
        for cat_name, cat_data in stats["negative_categories"].items():
            display_name = f"{cat_name}ï¼ˆå·®è¯„ï¼‰" if cat_name == "å…¶ä»–" else cat_name
            html += f"""
        <div class="category-item negative">
            <h3>{display_name}</h3>
            <p><strong>{cat_data['count']}</strong> æ¡è¯„è®º ({cat_data['percentage']:.1f}%)</p>
            """

            if cat_name in representative:
                html += "<h4>ä»£è¡¨æ€§è¯„è®º:</h4>"
                for review in representative[cat_name][:3]:
                    html += f"""
                    <div class="representative-review">
                        <div class="review-text">"{review['review_text'][:200]}{'...' if len(review['review_text']) > 200 else ''}"</div>
                        <div class="review-meta">
                            ğŸ‘ {review['votes_up']} ç‚¹èµ | 
                            â±ï¸ æ¸¸æˆæ—¶é•¿: {review.get('author_playtime_hours', 0):.1f}å°æ—¶
                        </div>
                    </div>
                    """

            html += "</div>"

        if stats["multi_category_stats"]:
            html += """
    </div>
    
    <div class="category-section">
        <h2>å¤šç±»åˆ«è¯„è®ºç»Ÿè®¡</h2>
        <p>AIè¯†åˆ«å‡ºçš„åŒ…å«å¤šä¸ªé—®é¢˜/ä¼˜ç‚¹çš„å¤åˆè¯„è®º:</p>
        """

            for cat_count, review_count in sorted(
                stats["multi_category_stats"].items()
            ):
                html += (
                    f"<p><strong>{cat_count}ä¸ªç±»åˆ«</strong>: {review_count} æ¡è¯„è®º</p>"
                )

        html += """
    </div>
    
    <div class="category-section">
        <h2>AIåˆ†æä¼˜åŠ¿</h2>
        <ul>
            <li><strong>è¯­ä¹‰ç†è§£</strong>: èƒ½ç†è§£ä¸Šä¸‹æ–‡å’Œéšå«æ„æ€</li>
            <li><strong>åè®½è¯†åˆ«</strong>: è¯†åˆ«é˜´é˜³æ€ªæ°”å’Œåè¯</li>
            <li><strong>å¤åˆåˆ†æ</strong>: ä¸€æ¡è¯„è®ºå¤šä¸ªé—®é¢˜å‡†ç¡®è¯†åˆ«</li>
            <li><strong>æƒ…æ„Ÿåˆ†æ</strong>: å‡†ç¡®åˆ¤æ–­çœŸå®æƒ…æ„Ÿå€¾å‘</li>
        </ul>
    </div>
</body>
</html>
        """

        return html


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="çº¯AI Steamè¯„è®ºåˆ†ç±»å·¥å…·")
    parser.add_argument("input_file", help="è¾“å…¥çš„è¯„è®ºCSVæ–‡ä»¶")
    parser.add_argument("--output", default=".", help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤å½“å‰ç›®å½•ï¼‰")

    args = parser.parse_args()

    if not os.path.exists(args.input_file):
        print(f"é”™è¯¯: è¾“å…¥æ–‡ä»¶ {args.input_file} ä¸å­˜åœ¨")
        return

    output_dir = args.output

    print("=== çº¯AI Steamè¯„è®ºåˆ†ç±»å·¥å…· ===")
    print(f"è¾“å…¥æ–‡ä»¶: {args.input_file}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print("-" * 50)

    # åŠ è½½æ•°æ®
    try:
        reviews_df = pd.read_csv(args.input_file)
        print(f"åŠ è½½äº† {len(reviews_df)} æ¡è¯„è®º")

    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•åŠ è½½CSVæ–‡ä»¶ - {e}")
        return

    # æ£€æŸ¥å¿…è¦å­—æ®µ
    required_fields = ["review_text", "voted_up"]
    missing_fields = [
        field for field in required_fields if field not in reviews_df.columns
    ]
    if missing_fields:
        print(f"é”™è¯¯: CSVæ–‡ä»¶ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_fields}")
        return

    # åˆå§‹åŒ–AIåˆ†ç±»å™¨
    try:
        classifier = ReviewAnalyzer()
    except ValueError as e:
        print(f"é”™è¯¯: {e}")
        print("è¯·ç¡®ä¿å·²æ­£ç¡®é…ç½®APIå¯†é’¥ç¯å¢ƒå˜é‡")
        return

    # APIè¿æ¥æµ‹è¯•
    if not classifier.test_api_connection():
        print("\nğŸš« DeepSeek APIè¿æ¥æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®åé‡è¯•")
        print("\nå¸¸è§é—®é¢˜æ’æŸ¥ï¼š")
        print("1. æ£€æŸ¥ .env æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”åŒ…å«æ­£ç¡®çš„ DEEPSEEK_API_KEY")
        print("2. ç¡®è®¤APIå¯†é’¥æ ¼å¼æ­£ç¡®ï¼ˆä»¥ sk- å¼€å¤´ï¼‰")
        print("3. éªŒè¯APIå¯†é’¥æ˜¯å¦æœ‰æ•ˆä¸”æœ‰è¶³å¤Ÿä½™é¢")
        print("4. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        return

    print("-" * 50)

    # è¿›è¡Œå¹¶è¡ŒAIåˆ†ç±»
    classified_df = classifier.classify_batch_parallel(reviews_df, None, output_dir)

    print("\n=== AIåˆ†æå®Œæˆ ===")
    print(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_dir}/classification_progress.json")
    print("")
    print("ğŸ“Š è¦ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Šï¼Œè¯·è¿è¡Œ:")
    print(
        f"poetry run python report_generator.py {output_dir}/classification_progress.json --output report"
    )
    print("")
    print("ğŸ’¡ æŠ¥å‘Šç”Ÿæˆå™¨ä¼šè‡ªåŠ¨æ£€æµ‹åŸå§‹CSVæ–‡ä»¶å¹¶ç”ŸæˆåŒ…å«å›¾è¡¨çš„HTMLæŠ¥å‘Š")


if __name__ == "__main__":
    main()
